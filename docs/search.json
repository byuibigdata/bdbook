[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Big Data Programming for Data Scientists",
    "section": "",
    "text": "Preface\nThis ‘book’ is primarily a place for the BYU-I data science program to store material used in their Big Data Class."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "As an introduction, we highlight the different elements of data science when using big data."
  },
  {
    "objectID": "intro/rules_thumb.html#the-spark-apis-let-you-use-your-language-of-preference",
    "href": "intro/rules_thumb.html#the-spark-apis-let-you-use-your-language-of-preference",
    "title": "4  Sark Rules of Thumb",
    "section": "4.1 The Spark APIs let you use your language of preference",
    "text": "4.1 The Spark APIs let you use your language of preference\nYou can use Java, Scala, Python, or R to access Spark. Like Goldilocks and the Three Bears, we want the language that is not ‘too hot’ or ‘too cold’ for data science use. Java is a bit too verbose for day-to-day data science work. Scala is fast but still a little verbose. Python is a little slower but ingrained in the data science community, and R is less easy to implement in a production environment.\n\npyspark (just right): The pyspark package looks to be the ‘just the right amount’ of the Spark APIs.\n\nsparkR (a little cold): Apache has developed an R package that is the official R connection to Spark.\nsparklyr (RStudio’s warm-up): If you are experienced with the Tidyverse, then RStudio’s sparklyr could pull you away from pyspark.\n\nYou can read a comparison of sparkR and sparklyr here."
  },
  {
    "objectID": "intro/rules_thumb.html#use-dataframes-ignore-rdds",
    "href": "intro/rules_thumb.html#use-dataframes-ignore-rdds",
    "title": "4  Sark Rules of Thumb",
    "section": "4.2 Use DataFrames (ignore RDDs)",
    "text": "4.2 Use DataFrames (ignore RDDs)\nFor day-to-day data science use, DataFrames are the option you should choose.\n\nSpark has built a framework to optimize Resilient Distributed Dataset (RDD) use when we program with DataFrame methods.\nSpark internally stores DataFrames in a binary format, so there is no need to serialize and deserialize data as it moves over the cluster.\n\nDatabricks provides a Deep Dive into Spark SQL’s Catalyst Optimizer and A Tale of Three Apache Spark APIs: RDDs vs. DataFrames and Datasets to help you understand more depth on the relationship between DataFrames.\nWe pulled the bullets and image below from the Databricks articles.\n\n\nIf you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\nIf you are an R user, use DataFrames.\nIf you are a Python user, use DataFrames and resort back to RDDs if you need more control."
  },
  {
    "objectID": "intro/rules_thumb.html#write-and-read-serialized-data-formats",
    "href": "intro/rules_thumb.html#write-and-read-serialized-data-formats",
    "title": "4  Sark Rules of Thumb",
    "section": "4.3 Write and Read serialized data formats",
    "text": "4.3 Write and Read serialized data formats\nThe Apache Parquet format is optimal for most data science applications. It is a serialized columnar format that provides speed and size benefits for big data applications. The following table compares the savings and the speedup obtained by converting data into Parquet from CSV.\n\n\n\n\n\n\n\n\n\n\nDataset\nSize on Amazon S3\nQuery Run Time\nData Scanned\nCost\n\n\n\n\nData stored as CSV files\n1 TB\n236 seconds\n1.15 TB\n$5.75\n\n\nData stored in Apache Parquet Format\n130 GB\n6.78 seconds\n2.51 GB\n$0.01\n\n\nSavings\n87% less when using Parquet\n34x faster\n99% less data scanned\n99.7% savings\n\n\n\nYou could use Avro with Spark as well. It is stored in rows, much like a .csv file, but is serialized."
  },
  {
    "objectID": "databricks.html",
    "href": "databricks.html",
    "title": "Databricks",
    "section": "",
    "text": "Databricks provides data scientists and data engineers a unified platform for scalable analytics and data management with almost unlimited storage and compute capacity through the use of Spark."
  },
  {
    "objectID": "databricks/databricks_intro.html#navigation",
    "href": "databricks/databricks_intro.html#navigation",
    "title": "5  What is Databricks?",
    "section": "5.1 Navigation",
    "text": "5.1 Navigation\n\nThis file provides a high-level overview of Databricks.\nThe HTML folder contains notebooks that can be imported into Databricks. All HTML files in that folder can be imported individually, or you can use examples.dbc to import them all at once."
  },
  {
    "objectID": "databricks/databricks_intro.html#the-elevator-pitches",
    "href": "databricks/databricks_intro.html#the-elevator-pitches",
    "title": "5  What is Databricks?",
    "section": "5.2 The elevator pitches",
    "text": "5.2 The elevator pitches\nCheck out these branded videos.\n\nIt’s time for DataBricks!\n\n\nDatabricks is the AI Company\n\nHere is a bit more nerdy DS/CS pitch from their website\n\nWith origins in academia and the open-source community, Databricks was founded in 2013 by the original creators of Apache Spark™, Delta Lake, and MLflow. As the world’s first and only lakehouse platform in the cloud, Databricks combines the best of data warehouses and data lakes to offer an open and unified platform for data and AI.\n\n\nIntroduction to Databricks Unified Data Platform: 5 min demo\n\n\n\n\nDatabricks comparison1"
  },
  {
    "objectID": "databricks/community_edition.html#community-edition-setup",
    "href": "databricks/community_edition.html#community-edition-setup",
    "title": "6  Databricks Community Edition",
    "section": "6.1 Community Edition Setup",
    "text": "6.1 Community Edition Setup\n\nCreate a community account on Databricks\nLogin into the Databricks community edition portal\nClick the compute icon on the left ()\nName your cluster\nCreate your cluster and then navigate to the libraries tab to install our needed Python packages (for example plotnine, altair). Pandas is already installed."
  },
  {
    "objectID": "databricks/community_edition.html#what-is-the-difference-between-the-databricks-community-edition-and-the-full-databricks-platform",
    "href": "databricks/community_edition.html#what-is-the-difference-between-the-databricks-community-edition-and-the-full-databricks-platform",
    "title": "6  Databricks Community Edition",
    "section": "6.2 What is the difference between the Databricks Community Edition and the full Databricks Platform?",
    "text": "6.2 What is the difference between the Databricks Community Edition and the full Databricks Platform?\n\nWith the Databricks Community Edition, the users will have access to 15GB clusters, a cluster manager and the notebook environment to prototype simple applications, and JDBC / ODBC integrations for BI analysis. The Databricks Community Edition access is not time-limited and users will not incur AWS costs for their cluster usage.\n\n\nThe full Databricks platform offers production-grade functionality, such as an unlimited number of clusters that easily scale up or down, a job launcher, collaboration, advanced security controls, and expert support. It helps users process data at scale, or build Apache Spark applications in a team setting.\n\n\nDatabricks"
  },
  {
    "objectID": "databricks/community_edition.html#using-databricks-notebooks",
    "href": "databricks/community_edition.html#using-databricks-notebooks",
    "title": "6  Databricks Community Edition",
    "section": "6.3 Using Databricks notebooks",
    "text": "6.3 Using Databricks notebooks\n\nWatch this video to see a short example of using the platform\nRead about the basics of Databricks notebooks\n\n\n6.3.1 Key links\n\nSign up for Community Edition\nA love-hate relationship with Databricks Notebooks\nDatabricks notebooks\nDatabricks Backstory"
  },
  {
    "objectID": "spark.html#links",
    "href": "spark.html#links",
    "title": "Spark",
    "section": "links",
    "text": "links\n\nSpark SQL, DataFrames and Datasets Guide\nChapter 4. Spark SQL and DataFrames"
  },
  {
    "objectID": "spark/spark_part_guide.html#data-science-programming",
    "href": "spark/spark_part_guide.html#data-science-programming",
    "title": "8  Using this Spark Guide",
    "section": "8.1 Data science programming",
    "text": "8.1 Data science programming\nEach language has their own view on the vocabulary of data munging. However, data science is starting to reach a point where each language is more like a dialect (British, Australian, or US English) than a unique language (Chinese, Finish, English). We are focusing on PySpark while providing dialect maps back to the top three data munging languages.\nOur first dialect discussion focuses on how each names the data object upon which we will write munging scripts. R has data.frame, Pandas has DataFrame, Pyspark has DataFrame, and SQL has TABLE to describe the rectangular rows and columns that so often represent how data is stored. In this book we will use the general dataframe to refer to all of them at once.\nEach of these data objects comes with column types or classes which facilitate data munging and analysis. Most of these types are very similar. However, they are not fully interchangable."
  },
  {
    "objectID": "spark/spark_part_guide.html#example-code-snippets",
    "href": "spark/spark_part_guide.html#example-code-snippets",
    "title": "8  Using this Spark Guide",
    "section": "8.2 Example Code Snippets",
    "text": "8.2 Example Code Snippets\nThe examples will show the similar code for each of the four most popular data munging languages to help you understand the mapping between each paradigm.\n\n8.2.1 Observable Example\n\nviewof language = Inputs.checkbox(\n  [\"Tidyverse\", \"Pandas\", \"SQL\", \"Pyspark\"], \n  { value: [\"Tidyverse\", \"Pandas\", \"Pyspark\"], \n    label: \"Display languages:\"\n  }\n)\n\n\n\n\n\n\n\nlanguage.includes(\"Pandas\")\n\n\n\n\n\n\n\nfunction checkBox(today, input) {\n    if(today.includes(input)) {\n      return input\n    } \n  }\n\n\nhtml`\n<div class=\"quarto-layout-panel\">\n<div class=\"quarto-layout-row quarto-layout-valign-top\">\n<section id=${checkBox(language, \"Tidyverse\")} class=\"level4 unnumbered quarto-layout-cell\" style=\"flex-basis: 50.0%;justify-content: center;\">\n<h4 class=\"unnumbered anchored\" data-anchor-id=${checkBox(language, \"Tidyverse\")}>${checkBox(language, \"Tidyverse\")}</h4>\n<div class=\"sourceCode\" id=\"cb8\"><pre class=\"sourceCode numberSource py number-lines code-with-copy\"><code class=\"sourceCode python\"><span id=\"cb8-1\"><a href=\"#cb8-1\"></a>df.select(<span class=\"st\">\"C1\"</span>, <span class=\"st\">\"C2\"</span>, <span class=\"st\">\"C3\"</span>, <span class=\"st\">\"C4\"</span>)</span></code><button title=\"Copy to Clipboard\" class=\"code-copy-button\"><i class=\"bi\"></i></button></pre></div>\n</section>\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`\n<div class=\"quarto-layout-panel\">\n<div class=\"quarto-layout-row quarto-layout-valign-top\">\n<section id=${checkBox(language, \"Tidyverse\")} class=\"level4 unnumbered quarto-layout-cell\" style=\"flex-basis: 50.0%;justify-content: center;\">\n<h4 class=\"unnumbered anchored\" data-anchor-id=${checkBox(language, \"Tidyverse\")}>${checkBox(language, \"Tidyverse\")}</h4>\n</section>\n`\n\n\n\n\n\n\n\n\n\n\n8.2.2 Selecting Columns from a dataframe\n\n\n\nPyspark\ndf.select(\"C1\", \"C2\", \"C3\", \"C4\")\n\n\nSQL\nSELECT C1, C2, C3, C4\nFROM df\n\n\n\n\nR::Tidyverse\ndf |> select(C1, C2, C3, C4)\n\n\nPy::Pandas\ndf.filter(items = [C1, C2, C3, C4])"
  },
  {
    "objectID": "spark/spark_intro.html#what-is-pyspark",
    "href": "spark/spark_intro.html#what-is-pyspark",
    "title": "9  What is Spark?",
    "section": "9.1 What is PySpark?",
    "text": "9.1 What is PySpark?\nPySpark supports the integration of Apache Spark and Python as a Python API for Spark. In addition, PySpark, helps you interface with Resilient Distributed Datasets (RDDs) in Apache Spark and Python programming language. PySpark refers to the collection of Python APIs for Spark.3 This book will focus on PySparkSQL and MLlib. Within Databricks we have access to the PySparkSQL methods using the spark object that is available in each notebook. Users of PySpark outside of Databricks would need to create their spark entry point with something like the following\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()"
  },
  {
    "objectID": "spark/features.html#spatial-unit-of-interest-derivation",
    "href": "spark/features.html#spatial-unit-of-interest-derivation",
    "title": "10  Feature Engineering",
    "section": "10.1 Spatial Unit of interest derivation",
    "text": "10.1 Spatial Unit of interest derivation\nThe tract_table has the county data within it. We can leverage this table to get our unit of analysis table."
  },
  {
    "objectID": "spark/features.html#label-creation",
    "href": "spark/features.html#label-creation",
    "title": "10  Feature Engineering",
    "section": "10.2 Label creation",
    "text": "10.2 Label creation\nThe idaho_temples.csv has our path to a label. We want to create a 0/1 label for has not/has a temple for each county."
  },
  {
    "objectID": "spark/features.html#feature-creation",
    "href": "spark/features.html#feature-creation",
    "title": "10  Feature Engineering",
    "section": "10.3 Feature Creation",
    "text": "10.3 Feature Creation\nWe want a clean feature that focuses on chapels for the Church of Jesus Christ of Latter-day Saints. How can we get a list of placekeys that are LDS chapels? Some regular expressions on the name will be helpful. What about day of the week attendance?\nWhat can we do to check the quality of the data."
  },
  {
    "objectID": "spark/features.html#temporal-considerations",
    "href": "spark/features.html#temporal-considerations",
    "title": "10  Feature Engineering",
    "section": "10.4 Temporal Considerations",
    "text": "10.4 Temporal Considerations\nThis data runs over COVID-19 as we have 2019 through 2022."
  },
  {
    "objectID": "spark/features.html#mapping-back-to-my-unit-of-analysis",
    "href": "spark/features.html#mapping-back-to-my-unit-of-analysis",
    "title": "10  Feature Engineering",
    "section": "10.5 Mapping back to my unit of analysis",
    "text": "10.5 Mapping back to my unit of analysis"
  },
  {
    "objectID": "training/training_intro.html#short-course-students",
    "href": "training/training_intro.html#short-course-students",
    "title": "12  Short Course Training Development",
    "section": "12.1 Short Course Students",
    "text": "12.1 Short Course Students\nShort course training is much different than the type of training you receive as a student in college. These professional pieces of training are meant to quickly provide an introduction, workable code snippets, and ways to push deeper after one or two hours. We have a small set of learning goals that we focus on to help the students get over the early-stage hurdles of the new technology. Professional short courses are filled with students with years of experience and multiple degrees, so they differ from a new first-year student in an introductory class. However, they are “freshmen” concerning the latest technology you introduce.\nThese learners will surprise you with the depth of their questions as well as the shallowness of their abilities. You need to be prepared to keep the material simple but have the background knowledge to clearly answer any depth questions that arise. This preparation requires you to have built an experience with the tool that would allow you to teach advanced material on the topic while staying focused on introductory teaching goals.\n\n\nIn college courses, our learning experiences are within a classroom that meets multiple times weekly for numerous months. That is often one part of a larger degree where multiple principles are taught beyond the technical tools of a new software package or technique. You have probably experienced a few different teaching styles from your professors. Your short course mustn’t be a 60-minute long lecture! Additionally, it is crucial that you take ownership of the material and that you provide clear examples that help the students use the new technology. They wouldn’t have come to your short course if they wanted to learn independently."
  },
  {
    "objectID": "training/training_intro.html#short-course-development",
    "href": "training/training_intro.html#short-course-development",
    "title": "12  Short Course Training Development",
    "section": "12.2 Short Course Development",
    "text": "12.2 Short Course Development\n\n12.2.1 The Goal\nExploring new technology without a guide or instructor is often a non-linear mess. Your experience as the initial explorer is different from what you want to provide to your students. You may explore multiple futile paths while finding a few clear and safe trails to give the best experience. After examining the varied approaches, you will ponder the destination and journey. What is the experience we want the student to have at completion? Is our path short enough to get them to that experience with enough time to enjoy the view?\n\n\n12.2.2 The Journey\nNow that you can see the end experience, you must use the right tools to help the students navigate the trail. Here are a few fundamental principles.\n\nGithub: Building your material on Github gives all the students rapid access to your training.\nPurpose/Introduction: Ensure your first few minutes frame the training. There is no need to sneak up on the students. Be direct and tell them the learning goals for your training. This part should take just a few minutes.\nOverviews: We must present context and details on the new technology. Expect to take time to “lecture” to get your class on the same page of understanding.\nActivities: These short courses focus on getting new tools into the hands of the student. Make sure you have hands-on activities where the students figure out how to use the tool in a real example. These activities are more than simple copy and paste. You want to give them a chance to think.\nWorking Examples: Make sure you have answers to your activities and other working examples for them to take home. The activities are often quite simple. I like to have one or two more complex examples for them to review after the class.\nBalance: It is important to balance the overviews and the activities. I ensure that my activities require as much time as any overview presentation material. It is also good to weave the overviews and balance them into shorter 5-10 blocks to provide variety.\n\n\n\n12.2.3 Completion\nThese short courses are valuable marketing tools. Make sure you have provided your contact information and that your students have references and examples that they can use to push their learning deeper. Think about how your material is organized in your Github repository so that others that find your space can learn and leverage your material without having participated in the course."
  },
  {
    "objectID": "training/training_intro.html#structuring-your-github-repository",
    "href": "training/training_intro.html#structuring-your-github-repository",
    "title": "12  Short Course Training Development",
    "section": "12.3 Structuring your Github Repository",
    "text": "12.3 Structuring your Github Repository"
  }
]